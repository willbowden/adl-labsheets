{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Data augmentation, Dropout, and Per-class Accuracy\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Data Augmentation](#Data-Augmentation)\n",
    "    1. [PyTorch Transformation Pipelines](#PyTorch-transformation-pipelines)\n",
    "    2. [Playing with datasets](#Playing-with-datasets)\n",
    "    3. [Online vs. Offline Augmentation](#Online-vs.-Offline-Augmentation)\n",
    "    4. [Training with Horizontal Flips](#Training-with-Horizontal-Flips)\n",
    "    5. [Training with Variable Brightness](#Training-with-Variable-Brightness)\n",
    "    6. [Training with Another Augmentation Technique](#Training-with-Another-Augmentation-Technique)\n",
    "2. [Dropout](#Dropout)\n",
    "3. [Per-class Accuracy](#Per-class-Accuracy)\n",
    "4. Optional Extension: [Model Checkpointing](#Optional-Extension:-Model-Checkpointing)\n",
    "\n",
    "Last lab session we looked at ways of reducing overfitting in our network. The methods we used were either tweaks to the optimisation process (tuning learning rate/batch size, using momentum) or adaptations to the network architecture (batch normalisation).  This week we will look at how we can improve our network's performance through the use of data augmentation techniques. That is, constructing more data from what we already have through the application of transformations (e.g. horizontal flipping, rotation, etc) that preserve the examples' labels. These transformations will introduce more variety into our training data to improve generalisation and reduce overfitting.\n",
    "\n",
    "We'll introduce one final network adaptation, *dropout*, to reduce overfitting.\n",
    "\n",
    "Finally we'll look at another way of measuring performance, *per-class accuracy*, which you'll be using in your coursework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Augmentation\n",
    "\n",
    "Data augmentation refers to the process of applying transforms to existing data that preserves the labels of dataset elements. Examples of this include horizontally flipping, cropping, and adding small amounts of noise.\n",
    "\n",
    "### Online vs. Offline Augmentation\n",
    "\n",
    "So how do we augment our training data? Your first thought might be to apply the transforms and save the new data, additionally training on this new data.\n",
    "This is called **offline augmentation**.\n",
    "An alternative approach is to load an unalterated example from the dataset and apply a transform (like flipping it, or adjusting its brightness) with some probability, this is called **online augmentation**.\n",
    "One of the major benefits of online augmentation is the greater diversity we can achieve in our data: each time the example is loaded it might be transformed in a different way. \n",
    "This is especially true when we apply multiple stochastic transforms one after the other.\n",
    "Offline augmentation is typically used when transform operations are very expensive and computing them online during training would massively slow down the training process due to waiting on the execution of the transformations.\n",
    "\n",
    "We'll only be using online augmentation in this lab as the transforms are relatively cheap to apply on the fly. Very few people working with DNNs operating on images use offline augmentation.\n",
    "\n",
    "### PyTorch transformation pipelines\n",
    "\n",
    "In PyTorch, datasets typically have a constructor parameter, `transform`, which you can use to inject an arbitrary transform that is applied to each data sample. Up to this point, our code uses this to convert images from [`PIL.Image.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html#the-image-class) objects to `torch.Tensor` objects by using the [`ToTensor`](https://pytorch.org/docs/1.2.0/torchvision/transforms.html#torchvision.transforms.ToTensor) transform as can be seen in the excerpt below:\n",
    "\n",
    "```python\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    args.dataset_root, train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    args.dataset_root, train=False, download=False, transform=transform\n",
    ")\n",
    "```\n",
    "\n",
    "Above, we only used a single transform, but it is very common to want to chain multiple transforms together into a pipeline.\n",
    "Torchvision provides a class, [`Compose`](https://pytorch.org/docs/1.2.0/torchvision/transforms.html#torchvision.transforms.Compose), to facilitate this. \n",
    "It is very simple, storing a list of transforms and applying one in turn when it is called on an input.\n",
    "\n",
    "For example, we could combine random horizontal flipping followed by converting the PIL Images to torch tensors like so:\n",
    "\n",
    "```python\n",
    "transform = Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "```\n",
    "\n",
    "\n",
    "### Playing with datasets\n",
    "\n",
    "Let's get some hands on experience and play around the [`CIFAR10`](https://pytorch.org/docs/1.2.0/torchvision/datasets.html#torchvision.datasets.CIFAR10) dataset and the `torchvision` transforms.\n",
    "\n",
    "**Task:** Open up a [colaboratory notebook](https://colab.research.google.com/) and run the following code\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10('data', download=True, train=True)\n",
    "img, label = dataset[1]\n",
    "print(type(img))\n",
    "print(label)\n",
    "img\n",
    "```\n",
    "\n",
    "Note that we **don't** set the `transform` argument of the [`CIFAR10`](https://pytorch.org/docs/1.2.0/torchvision/datasets.html#torchvision.datasets.CIFAR10) constructor; it will return the data without any transforms applied to it.\n",
    "This will enable us to easily play around with new transforms without having to reconstruct the dataset class with a different transform repeatedly.\n",
    "The [`CIFAR10`](https://pytorch.org/docs/1.2.0/torchvision/datasets.html#torchvision.datasets.CIFAR10) dataset reads images as [`PIL.Image.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html#the-image-class) objects by default.\n",
    "\n",
    "Have a look at what transforms `torchvision` provides for working with [`PIL.Image.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html#the-image-class) objects: https://pytorch.org/docs/1.2.0/torchvision/transforms.html#transforms-on-pil-image\n",
    "\n",
    "Let's test out the [`RandomHorizontalFlip`](https://pytorch.org/docs/1.2.0/torchvision/transforms.html#torchvision.transforms.RandomHorizontalFlip) transform.\n",
    "\n",
    "**Task:**  In a new cell construct a [`RandomHorizontalFlip`](https://pytorch.org/docs/1.2.0/torchvision/transforms.html#torchvision.transforms.RandomHorizontalFlip) transform \n",
    "```python\n",
    "from torchvision.transforms import RandomHorizontalFlip\n",
    "transform = RandomHorizontalFlip()\n",
    "transform(img)\n",
    "```\n",
    "\n",
    "Repeatedly rerun the cell and see how the image changes each time you run the cell. The transform is stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "<class 'PIL.Image.Image'>\n",
      "9\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwC7DrOk6Rdzz3s5FxuwY0UA9O7GpdE8X6NHqUEljpcLRNOFmuZCzsikfeBPGc8YFZei6FdSwyyxaVLLc7FDzGL5jkcnn1rQ1Pwjq0/hRglsyheiEhTnIxXQ4RtqzC7ubXj22hu9Kt7u1jgaMuzebCvPTkNivKGXaGEY2b2y3GNo+lev6haS2vhnQI5FaOWGWPzFJwSdpBz61x/iHwrGv2i8s4iWIy0Knt32/wCFcE5pS5TqUW48xf1P4sarpcrWkMekF4kUHYWbDcgqckcjFai+NNUvrRZ5dZ0iCLftw8LsrnAP3h05Ncpq3l2uoEy3Gjwi5bMRurXcxPf5u5z/ADqp4wmhj0exg8+3KIwLxpOEG4fxBew9q6vdexFn1Oq1TxlZzaKr6jd2RInIjksZDIrFRz1xg8jj3qeDWNB1W5hkh1a3aVlMYjEgBYHtg98146t/YaXNJFBN50LssiIRuYMVG7jtV2206S61mG9gtLp0wsgkx5exsg9T6ciuerh1J81zWFS0bWP/2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10('data', download=True, train=True)\n",
    "img, label = dataset[1]\n",
    "print(type(img))\n",
    "print(label)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDkFXcFEg37GyvGdw+ler+AraG00q4u7qOBYw6t5sy89OAua5fw94Vjb7PeXkRDAZWFj27bv8K7DT7SW68M6/HGrSSzSyeWoOSDtAGPSsITTlyluLUeYwdb8X6NJqU8l9pcKxLOVhuYyyM6gfeJHGc8YNRTazpOr3cE9lOTcbsCN1BPTswpdM8I6tB4UUPbMwbqgIY5yc1n61oV1FDFLLpUsVzsYJMIvmGBwePSu9QjbRnLd3Ogn1jQdKuZpJtWt1lVRGYzICVA7YHfNQaX4ys4dFZ9Ou7IEzgSSX0hjVSw46ZyeDx7V5jc6dJa6zNez2l0iYaQyY8ze2Seo9eBVJr+w1SaOKebyYUZpHQDaxYKdvHeuKlh1F81zsnUvG1j2dvGmqWNo08Ws6RPFv24SF1VDgn7x68isvTPixquqSraTR6QHlRgN5ZctwAowTyc1zvg+aGTR76Dz7cI7EpG84cbj/EV7j2q3pPl3WoAxXGjzC2bMptbXawPb5uxz/Kuj3VuZWfQ/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms import RandomHorizontalFlip\n",
    "transform = RandomHorizontalFlip()\n",
    "transform(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** what are some of the commonly used data augmentation techniques? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Training with Horizontal Flips\n",
    "\n",
    "Now that you've played around with horizontal flipping in Colaboratory, it's time to use it to augment our training data.\n",
    "\n",
    "**Task:** \n",
    "- Make a copy of last week's code\n",
    "- Add a [new option to the argument parser](https://docs.python.org/3/library/argparse.html#argparse.ArgumentParser.add_argument) in `train_cifar.py` called `--data-aug-hflip` with `action='store_true'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument(\"--data-aug-hflip\", action=\"store_true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If this argument (which is stored in the variable `args.data_aug_hflip`) is `True`, then apply the transform  before `ToTensor` using [`Compose`](https://pytorch.org/docs/1.2.0/torchvision/transforms.html#torchvision.transforms.Compose) to combine the transforms. Make sure to apply data augmentation transforms during training **only**.\n",
    "- Update `tb_log_dir_prefix` in `get_summary_writer_log_dir` to read \n",
    "  ```python\n",
    "    tb_log_dir_prefix = (\n",
    "        f\"CNN_bn_\"\n",
    "        f\"bs={args.batch_size}_\"\n",
    "        f\"lr={args.learning_rate}_\"\n",
    "        f\"momentum=0.9_\" +\n",
    "        (\"hflip_\" if args.data_aug_hflip else \"\") +\n",
    "        f\"run_\"\n",
    "    )\n",
    "  ```\n",
    "- Train the network with LR=1e-1, BS=128, momentum=0.9 and with horizontal flipping. Compare the final performance against your network trained without horizontal flipping. Did the data augmentation reduce overfitting of the network at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** when can horizontal flipping add no value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Training with Another Augmentation Technique\n",
    "\n",
    "**Task**:\n",
    "Pick one additional transformation from [`torchvision.transforms`](https://pytorch.org/docs/1.2.0/torchvision/transforms.html#transforms-on-pil-image) suitable for introducing more variation in training. \n",
    "Be careful to pick a transform that introduces diversity that is likely to be present in your test set.\n",
    "A bad choice would be to additionally train on grayscale images since the test data only contains color images.\n",
    "\n",
    "- *(Optionally)* experiment with the transform in your colaboratory notebook.\n",
    "- Add an option flag to the parser for this transform.\n",
    "- Update `tb_log_dir_prefix` to include some string that indicates what additional transform has been applied.\n",
    "- Train your network with only this new augmentation and compare against the performance of your network without any augmentation.\n",
    "- Note any impact on the severity of overfitting and the final test set performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Training with Variable Brightness \n",
    "\n",
    "\n",
    "Another way of introducing more variation into the training data is by randomly adjusting the brightness of images, making them either darker or brighter.\n",
    "Torchvision provides a transform called [`ColorJitter`](https://pytorch.org/docs/1.2.0/torchvision/transforms.html#torchvision.transforms.ColorJitter) suitable for this purpose.\n",
    "`ColorJitter` takes an argument `brightness` that defines how much to vary the brightness by; it should be set to a value between 0 and 1. \n",
    "The image's brightness will be adjusted by multiplying it by a factor sampled from $U[1-b, 1+b]$ where $b$ is the brightness factor passed in the constructor. Afterwards, the new pixel values are clipped to lie in the range $[0, 255]$.\n",
    "\n",
    "\n",
    "**Task**: \n",
    "- Add a [new option to the argument parser](https://docs.python.org/3/library/argparse.html#argparse.ArgumentParser.add_argument) in `train_cifar.py` called `--data-aug-brightness` with the default `0` and type `float`.\n",
    "- Construct a [`ColorJitter`](https://pytorch.org/docs/1.2.0/torchvision/transforms.html#torchvision.transforms.ColorJitter) transform and add it to the training transform. \n",
    "- Update `tb_log_dir_prefix` in `get_summary_writer_log_dir` to read:\n",
    "  ```python\n",
    "    tb_log_dir_prefix = (\n",
    "        f\"CNN_bn_\"\n",
    "        f\"bs={args.batch_size}_\"\n",
    "        f\"lr={args.learning_rate}_\"\n",
    "        f\"momentum=0.9_\"\n",
    "        f\"brightness={args.data_aug_brightness}_\" +\n",
    "        (\"hflip_\" if args.data_aug_hflip else \"\") +\n",
    "        f\"run_\"\n",
    "    )\n",
    "    ```\n",
    "- Retrain the network varying the amount of brightness jittering, try the following values: 0.05, 0.1, 0.3. Use BS=128, LR=1e-1, momentum=0.9, and don't use horizontal flipping. Does this technique help reduce overfitting or improve generalisation?\n",
    "- Retrain the network with brightness jittering of 0.1, but this time *also* include horizontal flipping as an augmentation technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** can excess color jittering hurt performance? if so, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Per-class accuracy\n",
    "\n",
    "So far we've used a single metric, *accuracy*, to measure the performance of your model on either the training or test set. Whilst this is useful to get an overall picture of performance, it is rather uninformative when it comes to understanding where the model does well or falls down. We can gain a more nuanced understanding of the model's performance by computing the accuracy per-class (this is a slight misnomer as technically this is per-class recall).\n",
    "\n",
    "Previously, we've computed accuracy by\n",
    "\n",
    "$$ \\mathrm{Acc} = \\frac{\\sum_i \\operatorname{I}[\\hat{y}_i = y_i]}{N}$$\n",
    "\n",
    "where \n",
    "- $\\hat{y}_i$ is the model's classification of example $i$.\n",
    "- $y_i$ is the label of example $i$.\n",
    "- $\\operatorname{I}[\\cdot]$ is the indicator function that returns 1 if its argument is true, and 0 otherwise.\n",
    "- $N$ is the total number of examples.\n",
    "\n",
    "We can compute this on a per-class basis by looking at the set of examples labelled with class $C$ and counting how many were correctly classified divided by the total number examples of $C$:\n",
    "\n",
    "$$ \\mathrm{Acc}_C = \\frac{\\sum_i \\operatorname{I}[\\hat{y}_i = y_i = C]}{\\sum_i \\operatorname{I}[y_i = C]} $$\n",
    "\n",
    "**Task:** Implement a function to compute per-class accuracy and print out the per-class accuracy after every validation step. The network's logit indices correspond to the classes in alphabetic order, i.e. logit 0 → airplane, logit 1 → automobile, ..., logit 9 → truck. The classes in CIFAR10 can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "There are many different ways to implement this. If you need a hint to get you started, you can use `==` to obtain a boolean mask indicating whether corresponding elements have the same class. This could be used to compute how many labels correspond to the class of interest in addition to how many labels are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** When can computing the accuracy lead to a biased estimation of the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dropout\n",
    "\n",
    "A popular regularisation technique is called **dropout**.\n",
    "Dropout prevents overfitting by randomly removing inputs to a layer. This means that the layer can't rely on a large number of inputs always being present to recognise a specific concept and has to instead rely on fewer combinations of inputs to detect these concepts.\n",
    "The [original paper](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf) puts forward a good explanation:\n",
    "\n",
    "> Deep neural networks contain multiple non-linear hidden layers and this makes them very expressive models that can learn very complicated relationships between their inputs and outputs. With limited training data, however, many of these complicated relationships will be the result of sampling noise, so they will exist in the training set but not in real test data even if it is drawn from the same distribution.  This leads to overfitting ...\n",
    "\n",
    "Dropout stochastically removes input units, resulting in those units having no effect. Each unit is independently dropped-out with probability $p$. The effect of dropout on the network structure can be seen in the figure below:\n",
    "\n",
    "![Dropout networks](./media/dropout.png)\n",
    "\n",
    "Setting input units to 0 has the same effect as removing the edges between output neurons and the input units that have been dropped out.\n",
    "\n",
    "Typically, dropout is implemented by sampling a *dropout mask* $\\mathbf{m}$ from a multivariate [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) and using this to gate the inputs.\n",
    "For a layer with $D$ inputs, $D$ binary samples are drawn from $\\mathrm{B}(1-p)$ (a Bernoulli distribution with mean $1-p$) to form $\\mathbf{m} \\in \\{0, 1\\}^D$. The inputs $\\mathbf{x} \\in \\mathbb{R}^D$ are then gated by the dropout mask $\\mathbf{x}\\odot \\mathbf{m}$ (where $\\odot$ represents elementwise multiplication) before being passed into the layer.\n",
    "As previously stated, this only happens during training, and at test time the inputs are left untouched.\n",
    "\n",
    "One issue we haven't noted with the above formulation is the difference in input distributions to the layer during training vs. testing.\n",
    "Consider dropping out the inputs to a fully connected layer. \n",
    "Let's look at one of the neurons in the layer, each of which are connected to the $D$ inputs.\n",
    "The expected input to a neuron during training will be \n",
    "$$\\mathbb{E}[\\mathbf{m} \\odot \\mathbf{x}] = \\mathbb{E}[\\mathbf{m}] \\odot \\mathbb{E}[\\mathbf{x}] = (1-p) \\mathbb{E}[\\mathbf{x}]$$\n",
    "but during testing it will instead be $\\mathbb{E}[\\mathbf{x}]$.\n",
    "So, the expected difference between training and testing in the input to the neuron is $\\frac{1}{1-p}$.\n",
    "This is easily resolvable by rescaling the inputs during training, we simply weight the inputs by $\\frac{1}{1-p}$ during training and leave them untouched during testing. This keeps the expected value of the inputs the same during testing or training.\n",
    "\n",
    "\n",
    "See the [original paper](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf) for more details, it is eminently readable. Be aware that they define $p$ to be the probability that a neuron is retained, rather than dropped, most frameworks these days define it like we have defined above. The scaling used to keep the expected value of the inputs the same during training and testing also differs to that above, but achieves the same end.\n",
    "\n",
    "Dropout is typically applied on the input to FC layers, but can also be applied to the input of any hidden layer.\n",
    "Typical dropout probabilities are 0.5--0.8 for FC layers, but much lower for convolutional layers such as 0.1.\n",
    "\n",
    "**Task:** \n",
    "- Add a [new option flag](https://docs.python.org/3/library/argparse.html#argparse.ArgumentParser.add_argument) `--dropout` with a default value of 0 and type `float`. \n",
    "- Apply dropout to the input of each fully connected layer\n",
    " - Make a new constructor argument called `dropout` in the `CNN` class.\n",
    " - Pass in the dropout value stored in `args.dropout` when constructing the `CNN` class.\n",
    " - In `CNN`'s constructor, make a [`nn.Dropout`](https://pytorch.org/docs/1.2.0/nn.html#torch.nn.Dropout) object constructed with the `dropout` parameter, and assign it to `self.dropout`.\n",
    " - In the `forward` method, apply `self.droput` to the input of each fully connected layer.\n",
    "- Update `tb_log_dir_prefix` in `get_summary_writer_log_dir` to read:\n",
    "  ```python\n",
    "    tb_log_dir_prefix = (\n",
    "        f\"CNN_bn_\"\n",
    "        f\"dropout={args.dropout}_\"\n",
    "        ...\n",
    "    )\n",
    "    ```\n",
    "- Train the model with BS=128, LR=1e-1, momentum=0.9 and $p = 0.5$ (the probability of dropping out an input) \n",
    "\n",
    "\n",
    "As always, here are the results when we scanned across a range of droput values:\n",
    "\n",
    "| Dropout $p$   | Train accuracy (%)   | Test accuracy (%)   | Train-test accuracy gap (%)   |\n",
    "| ------------- | -------------------- | ------------------- | ----------------------------- |\n",
    "| 0.0           | 100.00               | 76.56               | 23.44                         |\n",
    "| 0.1           | 99.60                | 76.31               | 23.29                         |\n",
    "| 0.2           | 98.33                | 75.99               | 22.34                         |\n",
    "| 0.3           | 95.42                | 78.09               | 17.33                         |\n",
    "| 0.4           | 92.46                | 76.38               | 16.08                         |\n",
    "| 0.5           | 84.96                | 77.90               | 7.06                          |\n",
    "| 0.6           | 77.21                | 79.60               | -2.39                         |\n",
    "| 0.7           | 71.17                | 70.55               | 0.62                          |\n",
    "| 0.8           | 59.60                | 61.73               | -2.13                         |\n",
    "| 0.9           | 17.10                | 19.53               | -2.43                         |\n",
    "\n",
    "Notice how dropout has a profound effect on reducing the training accuracy, it acts as an excellent regulariser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** what are the pros and cons of using dropout compared to other regularization methods such as L2 norm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional Extension: Model Checkpointing\n",
    "\n",
    "Training networks can take a long time, sometimes even weeks. It is not feasible to continually retrain networks when you want to evaluate them. To avoid this we perform *checkpointing* which is the process of saving model parameters every so often so that if our script crashes we can resume training and more importantly, separate the evaluation of the trained network from the training process itself. \n",
    "\n",
    "In PyTorch, the parameters of a model can be obtained by calling [`model.state_dict()`](https://pytorch.org/docs/1.2.0/nn.html#torch.nn.Module.state_dict), this returns a dictionary mapping between parameter names and tensors containing those parameters. You can save this dictionary to a file using [`torch.save`](https://pytorch.org/docs/1.2.0/torch.html#torch.save), read it from disk using [`torch.load`](https://pytorch.org/docs/1.2.0/torch.html#torch.load), and finally load it back into the network using [`model.load_state_dict(state_dict)`](https://pytorch.org/docs/1.2.0/nn.html#torch.nn.Module.load_state_dict)\n",
    "\n",
    "A sketch of the general set up of the checkpointing process is given below (this is *not* complete code, but shows you the outline of the general process):\n",
    "\n",
    "```python\n",
    "parser.add_argument(\"--checkpoint-path\", type=Path)\n",
    "parser.add_argument(\"--checkpoint-frequency\", type=int, default=1, help=\"Save a checkpoint every N epochs\")\n",
    "parser.add_argument(\"--resume-checkpoint\", type=Path)\n",
    "parser.add_argument(\"--start-epoch\", type=int, default=0)\n",
    "\n",
    "if args.resume_checkpoint.exists():\n",
    "    state_dict = torch.load(args.resume_checkpoint)\n",
    "    print(f\"Loading model from {args.resume_checkpoint}\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_for_one_epoch(model)\n",
    "    # Save every args.checkpoint_frequency or if this is the last epoch\n",
    "    if (epoch + 1) % args.checkpoint_frequency == 0 or (epoch + 1) == epochs:\n",
    "        print(f\"Saving model to {args.checkpoint_path}\")\n",
    "        torch.save(model.state_dict(), args.checkpoint_path)\n",
    "```\n",
    "\n",
    "\n",
    "**TASK**: Using the above example code, implement model checkpointing in your code.\n",
    "\n",
    "\n",
    "You can save any object using `torch.save`. It is useful to persist more than just your model parameters. For example you could store your command line args so you can determine what hyperparameters were used to train that particular model. Another good idea is to save the accuracy the model achieves in validation, e.g.:\n",
    "\n",
    "```python\n",
    "torch.save({\n",
    "    'args': args,\n",
    "    'model': model.state_dict(),\n",
    "    'accuracy': accuracy\n",
    "}, args.checkpoint_path)\n",
    "```\n",
    "\n",
    "then when you load the checkpoint you will have to pull out the state dict to load into the model:\n",
    "\n",
    "```python\n",
    "checkpoint = torch.load(args.resume_checkpoint)\n",
    "print(f\"Resuming model {args.resume_checkpoint} that achieved {checkpoint['accuracy']}% accuracy\")\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "```\n",
    "\n",
    "**TASK**: Write an additional script that loads your checkpoint and evaluates it on the test set. You will probably want to split up your python code into multiple files so that the CNN class resides in its own file and the training and testing scripts import this definition.\n",
    "\n",
    "You might find the [PyTorch tutorial on checkpointing](https://pytorch.org/tutorials/beginner/saving_loading_models.html) a useful resource in implementing this extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** what are some additional criteria you need to consider if you are saving checkpoints while training in a distributed manner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comsm0045",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
